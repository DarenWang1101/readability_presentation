---
title: "æ¼¢å­—åœã¨éæ¼¢å­—åœæ—¥æœ¬èªæ•™ç§‘æ›¸ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ç ”ç©¶"
subtitle:  "ãƒ¼é•·æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’å¯¾è±¡ã«ãƒ¼"
author: "ç‹ã€€ç°«å½±"
date: "2023-11-03"
institute: "å¤§é˜ªå¤§å­¦ äººæ–‡å­¦ç ”ç©¶ç§‘è¨€èªæ–‡åŒ–ç ”ç©¶å°‚æ”»M2"
execute:
  echo: true
from: markdown+emoji
format: 
  revealjs:
    # theme: solarized
    # smaller: true
    # scrollable: true
    slide-number: true
    preview-links: auto
    incremental: false
    footer: ":book:"
    # code-overflow: wrap
    # revealjs-plugins:
    #   - chalkboard
    # include-in-header:
    #   - text: 
    #       \usepackage{physics}
    html-math-method:
      method: mathjax
      url: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML"
jupyter: python3
# pdf-engine: lualatex
# documentclass: ltjsarticle
# classoption: lualatex,ja=standard
# editor: visual
---

# ç›®æ¬¡

::: incremental
1. ç ”ç©¶èƒŒæ™¯
2. è‹±èªãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®å…ˆè¡Œç ”ç©¶
3. æ—¥æœ¬èªãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®å…ˆè¡Œç ”ç©¶
4. ãƒªã‚µãƒ¼ãƒã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³
5. ãƒ‡ãƒ¼ã‚¿ã¨ã‚³ãƒ¼ãƒ‘ã‚¹ç´¹ä»‹
6. æ–¹æ³•ã¨çµæœ
7. å‚è€ƒæ–‡çŒ®
:::

<!-- ## ç ”ç©¶èƒŒæ™¯ğŸ“Œ

::: {.incremental}
- ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®å®šç¾©
- ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®ç™ºå±•
- ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®ç ”ç©¶æ‰‹æ³•
:::

:::footer
:bookmark:
::: -->

## ç ”ç©¶èƒŒæ™¯ğŸ“Œ {auto-animate="true" .smaller}

::: {.fragment fragment-index=1}
::: {.fragment .highlight-blue}
- ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®å®šç¾©
:::
:::

::: {.fragment fragment-index=1}
- ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®ç™ºå±•
:::

::: {.fragment fragment-index=1}
- ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®ç ”ç©¶æ‰‹æ³•
:::

. . .

<br/>

### [ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£](https://en.wikipedia.org/wiki/Readability)ã¨ã¯â“
ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã¯ç ”ç©¶è€…ã‚„ç ”ç©¶ç›®çš„ã«ã‚ˆã£ã¦å¤šæ§˜ã«å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã€‚ä¾‹ãˆã°ï¼š

::: {.nonincremental}
- surface-level indicators of text difficulty that can be calculated
- complex cognitive processes that take place when reading a text are included
:::

<!-- ã“ã“ã§ã¯ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã‚’ãƒ†ã‚­ã‚¹ãƒˆã®èª­ã¿ã‚„ã™ã•ã€ã¾ãŸã¯èª­ã¿ã«ãã•ã«æŒ‡æ¨™ã‚’è¨­å®šã™ã‚‹ã“ã¨ãŒã§ãã€æ¸¬ å®šã§ãã‚‹ã‚‚ã®ã¨ã—ã¦å®šç¾©ã™ã‚‹ã€‚ -->

:::footer
:bookmark:
:::


## ç ”ç©¶èƒŒæ™¯ğŸ“Œ {auto-animate="true" .smaller}

### [ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£](https://en.wikipedia.org/wiki/Readability)ã¨ã¯â“
ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã¯ç ”ç©¶è€…ã‚„ç ”ç©¶ç›®çš„ã«ã‚ˆã£ã¦å¤šæ§˜ã«å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã€‚ä¾‹ãˆã°ï¼š

::: {.nonincremental}
- surface-level indicators of text difficulty that can be calculated
- complex cognitive processes that take place when reading a text are included
:::

ã“ã“ã§ã¯ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã‚’ãƒ†ã‚­ã‚¹ãƒˆã®èª­ã¿ã‚„ã™ã•ã€ã¾ãŸã¯èª­ã¿ã«ãã•ã«æŒ‡æ¨™ã‚’è¨­å®šã™ã‚‹ã“ã¨ãŒã§ãã€æ¸¬å®šã§ãã‚‹ã‚‚ã®ã¨ã—ã¦å®šç¾©ã™ã‚‹ã€‚

:::footer
:bookmark:
:::


## ç ”ç©¶èƒŒæ™¯ğŸ“Œ  {auto-animate="true" .smaller}

::: {.fragment fragment-index=1}
- ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®å®šç¾©
:::

::: {.fragment fragment-index=1}
::: {.fragment .highlight-blue}
- ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®ç™ºå±•
:::
:::

::: {.fragment fragment-index=1}
- ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®ç ”ç©¶æ‰‹æ³•
:::

. . .

<br/>

### ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®ç™ºå±•
ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã¯19ä¸–ç´€60å¹´ä»£ã‹ã‚‰ç™ºå±•ã—ã¤ã¥ã€å­¦è¡“ä»¥å¤–ã«ã‚‚æ•™è‚²ã‚„å‡ºç‰ˆãªã©ã®é ˜åŸŸã§å¹…åºƒãç ”ç©¶ã•ã‚Œã¦ã„ã‚‹ã€‚

è‹±èªãŒä¸»ä½“ãªæ™‚æœŸã‚’çµŒã£ã¦ã€ã‚¢ã‚¸ã‚¢åœã§ã¯æ—¥æœ¬èªã€ä¸­å›½èªã€ãªã©ã‚‚ãã‚Œã‚’è¸ã¾ãˆã¦ã€æ¬§ç±³è¨€èªã®ç ”ç©¶ã‚’ç¶™æ‰¿ã—ãŸä¸Šã§ã€ãã‚Œãã‚Œè¨€èªã®ç‰¹å¾´ã‚’ç„¦ç‚¹ã«ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ç ”ç©¶ã‚’å±•é–‹ã—ã¦ã„ã‚‹

:::footer
:bookmark:
:::



## ç ”ç©¶èƒŒæ™¯ğŸ“Œ  {auto-animate="true" .smaller}

::: {.fragment fragment-index=1}
- ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®å®šç¾©
:::

::: {.fragment fragment-index=1}
- ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®ç™ºå±•
:::

::: {.fragment fragment-index=1}
::: {.fragment .highlight-blue}
- ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®ç ”ç©¶æ‰‹æ³•
:::
:::
. . .

<br/>

### ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®ç ”ç©¶æ‰‹æ³•

- traditional methods
- methods inspired by cognitive science
- methods based on the use of statistical language modeling tools

è¨€èªè³‡æºãŒãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã•ã‚Œã‚‹ã«ã¤ã‚Œã¦ã€...

- methods based on the use of large language models  ... â“â—

:::footer
:bookmark:
:::


## ç ”ç©¶èƒŒæ™¯ğŸ“Œ  {auto-animate="true" .smaller}

### ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®ç ”ç©¶æ‰‹æ³•

- traditional methods
- methods inspired by cognitive science
- methods based on the use of statistical language modeling tools

è¨€èªè³‡æºãŒãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã•ã‚Œã‚‹ã«ã¤ã‚Œã¦ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã‚‚ã—ãã¯ä¸€éƒ¨ã®ã‚¨ãƒ‡ã‚£ã‚¿ãŒè‡ªå‹•çš„ã‹ã¤ãƒã‚¤ã‚¹ãƒ”ãƒ¼ãƒ‰ã§èªã®é »åº¦é›†è¨ˆã‚„æ–‡é•·ã€èªã®é•·ã•ãªã©ã®æƒ…å ±ã‚’æç¤ºã—ã¦ãã‚Œã‚‹ã€‚

èªçŸ¥ç§‘å­¦ç†è«–ã§ç°¡å˜ã«é›†è¨ˆã§ãã‚‹æŒ‡æ¨™ã ã‘ã§ã¯ãªãã€æ®µè½ã®é–“ã®çµåˆå…·åˆã‚„ãªã©ã¨ã„ã£ãŸå†…å®¹ã®ã¾ã¨ã¾ã‚ŠçŠ¶æ³ã‚’æ¸¬é‡ã™ã‚‹æŒ‡æ¨™ã¨æ–‡æ³•ã®é›£æ˜“åº¦ã‚’ã¯ã‹ã‚‹æŒ‡æ¨™ãªã©ãŒé–‹ç™ºã•ã‚Œã‚‹ã€‚

ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®æ€¥é€Ÿãªç™ºå±•ã¨å…±ã«ã€çµ±è¨ˆè¨€èªãƒ¢ãƒ‡ãƒ«(SLMs)ã‚„ã€æ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•(ML)ã‚’å¿œç”¨ã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ (SVMs)ãŒæ–°ãŸãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒæç¤ºã™ã‚‹ã€‚

:::footer
:bookmark:
:::


## è‹±èªãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®å…ˆè¡Œç ”ç©¶ğŸ¤ {auto-animate="true" .smaller}

::: {.r-fit-text}
> The assumptions underlying traditional readability formulas are far too simplistic to account for the varied linguistic and textual factors that can differ widely, especially in adult texts (Bailin and Grafstein 2001; Schriver 2000).

> Because of the rapid influx of newly designed procedures for analyzing text difficulty and matching readers and texts, educational researchers may either feel overwhelmed with the numerous options or may simply be tempted to stick with the classic methods they and their predecessors have used for decades. However, selecting appropriate texts for a population of readers requires some understanding of both the reader and the text, and different methods may be more or less appropriate for different types of texts and different populations of readers.(Benjamin, 2012)
:::

- äººæ°—ã®ã‚ã‚‹å¾“æ¥æŒ‡æ¨™ã¨æ¬¡ã€…ã¨ç¾ã‚Œå°½ããªã„æ–°ãŸãªãƒ¡ã‚½ãƒƒãƒ‰
- ãƒ†ã‚­ã‚¹ãƒˆã®ç¨®é¡ã¨èª­è€…ã®ç‰¹å¾´
- é©åˆ‡ãªãƒãƒƒãƒãƒ³ã‚°ã‚’ã™ã‚‹ã«ã¯ãƒ†ã‚­ã‚¹ãƒˆã¨å¯¾è±¡èª­è€…ã«å¯¾ã™ã‚‹æ´å¯ŸãŒå¿…è¦

:::footer
:bookmark_tabs:
:::


## è‹±èªãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®å…ˆè¡Œç ”ç©¶ğŸ¤ {auto-animate="true" .smaller}

- äººæ°—ã®ã‚ã‚‹å¾“æ¥æŒ‡æ¨™ã¨æ¬¡ã€…ã¨ç¾ã‚Œå°½ããªã„æ–°ãŸãªãƒ¡ã‚½ãƒƒãƒ‰

::: {.fragment}
- GFI/Guning fog index(1952) 

::: {style="text-align: center;"}
$$
GFI = 0.4(\frac{totalWords}{totalSentences} + 100\frac{longWords}{totalSentences})
$$
:::
:::


:::{.fragment}
- FRE/Flesch reading ease(1975)

::: {style="text-align: center;"}
$$
FRE = 206.835-1.15(\frac{totalWords}{totalSentences}-84.6\frac{totalSyllables}{totalWords})
$$
:::
:::

<!-- 
  - FKGL/Flesch-Kincaid grade level(1975)
$$
FKGL = 0.39(\frac{totalWords}{totalSentences})+11.8(\frac{totalSyllables}{totalWords})-15.59
$$ -->

::: {.fragment}

- ARI/Automated readability index(1967)

::: {style="text-align: center;"}
$$
ARI= 4.71(\frac{totalCharacters}{totalWords})+0.5(\frac{totalWords}{totalSentences})-21.43
$$
:::
:::

  <!-- - DCRF/the Dale-Chall readability formula(1948)
$$
DCRF = 0.1579(\frac{totalCharacters}{totalWords})+0.5(\frac{totalWords}{totalSentences})
$$ -->

  <!-- - SMOG/simple measure if Gobbledygook(1969)
$$
SMOG = 1.0430\sqrt{numberOfPolysyllables\frac{30}{totalSentences}}+3.1291
$$ -->


:::

:::footer
:bookmark_tabs:
:::


## è‹±èªãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®å…ˆè¡Œç ”ç©¶ğŸ¤ {auto-animate="true" .smaller}

::: {.r-fit-text}
- å¾“æ¥æŒ‡æ¨™

wide used, simple and popular but largly effected by the numbers of characters, words, sentences and syllables.

> For example, if we compare the Newsela corpus and Slovenian SB corpus, which both cover roughly the same age group, we can see that for some readability indicators (FRE, FKGL, DCRF, and ASL) the values are on entirely different scales.(Martinc et al., 2021)

- èªçŸ¥ç§‘å­¦ç†è«–ã®å•“ç™ºã‹ã‚‰æ–°ãŸãªæ¸¬é‡ãƒ¡ã‚½ãƒƒãƒ‰

> As connectionist (McClelland and Rumelhart 1981; Rumelhart and McClelland 1982), schema (e.g., Anderson and Pichert 1978), prototype (Rosch et al. 1976), and spreading activation (Anderson 1983) theories emerged to explain how humans store and retrieve information in long-term memory, some researchers who studied text processing began to hypothesize that text difficulty and readability were more related to coherence and the relationships between elements in a text rather than simply the sum or averages of individual surface features.(Benjamin, 2012)
:::

:::footer
:bookmark_tabs:
:::

## è‹±èªãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®å…ˆè¡Œç ”ç©¶ğŸ¤ {auto-animate="true" .smaller}

- èªçŸ¥ç§‘å­¦ç†è«–ã®å•“ç™ºã‹ã‚‰æ–°ãŸãªæ¸¬é‡ãƒ¡ã‚½ãƒƒãƒ‰




:::footer
:bookmark_tabs:
:::


## æ—¥æœ¬èªãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£ã®å…ˆè¡Œç ”ç©¶ğŸ¤ 


(Shibasaki et al., 2010) :

::: {.r-fit-text}
year = -147.9 + (3.601E-4 * (hiragana / characters)) - (8.772E-2 * (hiragana / characters)^2) + (6.651 * (hiragana / characters)) + (3.679 * chunks) + (3.142E-4 * hiragana^2 * characters) - (3.986E-4 * hiragana^2 * chunks) - (3.207E-4 * characters^2) - (3.109E-2 * (hiragana / characters)) - (7.968E-3 * chunks^2) + (3.468E-3 * hiragana * characters * chunks)

yearÂ =Â å­¦å¹´Â (thisÂ isÂ theÂ returnÂ valueÂ ofÂ theÂ function).  
hiragana / characters = hiragana-ratioÂ =Â å¹³ä»®åã®å‰²åˆ.
charactersÂ =Â æ–‡ã®å¹³å‡æ–‡å­—æ•°.   chunksÂ =Â æ–‡ã®å¹³å‡æ–‡ç¯€æ•°   predicatesÂ =Â æ–‡ã®å¹³å‡è¿°èªæ•°

(R2=.690ã€èª¿æ•´æ¸ˆã¿R2=.681ã€AIC=1409.201)
:::

<!-- <br/> -->


(æåœ¨é¬, 2016) :

::: {.r-fit-text}
æ–‡ç« ã®èª­ã¿ã‚„ã™ã•={å¹³å‡æ–‡é•· * -0.056}+{æ¼¢èªç‡ * -0.126}+{å’Œèªç‡ * -0.042}+{å‹•è©ç‡ * -0.145}+{åŠ©è©ç‡ * -0.044}+11.724ã€

(R2=.896) 
:::



:::footer
:bookmark_tabs:
:::

## ãƒªã‚µãƒ¼ãƒã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³ğŸ’¡



## ãƒ‡ãƒ¼ã‚¿ç´¹ä»‹ğŸ“ {auto-animate="true" .smaller}


![](img1.png){.absolute top="100" left="50"}

![](img2.png){.absolute top="200" right="50"}

![](img3.png){.absolute bottom="0" left="400"}


:::footer
:books:
:::


## ãƒ‡ãƒ¼ã‚¿ç´¹ä»‹ğŸ“ {auto-animate="true" .smaller}


![](img1.png){.absolute top="200" left="50"}

:::footer
:books:
:::

## ãƒ‡ãƒ¼ã‚¿ç´¹ä»‹ğŸ“ {auto-animate="true" .smaller}


![](img2.png){.absolute top="200" left="50"}

:::footer
:books:
:::

## ãƒ‡ãƒ¼ã‚¿ç´¹ä»‹ğŸ“ {auto-animate="true" .smaller}


![](img3.png){.absolute top="200" left="50"}

:::footer
:books:
:::




## ã‚³ãƒ¼ãƒ‘ã‚¹æ§‹ç¯‰ğŸ“¦

:::: {.columns}

::: {.column width="65%"}
::: {.fragment fragment-index=1}
#### 1. ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’åé›†
:::

::: {.fragment fragment-index=2}
::: {style="text-align: left;"}
â¬
:::
:::

::: {.fragment fragment-index=3}
#### 2. .pngã‚’.txtã«

###### OCRã¨å‰å‡¦ç†
:::

::: {.fragment fragment-index=4}
::: {style="text-align: left;"}
â¬
:::
:::

::: {.fragment fragment-index=5}
#### 3. ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ§‹é€ åŒ–ã«

###### åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ãƒ¡ã‚¿æƒ…å ±ã‚’è¨˜å…¥
:::
:::

::: {.column width="35%"}
::: {.fragment fragment-index=6}
*SomethingNew*
:::
:::

::::


:::footer
:file_folder:
:::



## ã‚³ãƒ¼ãƒ‘ã‚¹ç´¹ä»‹ğŸ“¦ {.smaller}

#### æ•™ç§‘æ›¸ã‚³ãƒ¼ãƒ‘ã‚¹ã®æƒ…å ±:

![](image.png){.absolute }


<!-- ## ã‚³ãƒ¼ãƒ‘ã‚¹ç´¹ä»‹ {.smaller}

#### æ•™ç§‘æ›¸ã‚³ãƒ¼ãƒ‘ã‚¹ã®æƒ…å ±:

<br/> -->

<!-- 
::: columns -->

<!-- ::: {.r-fit-text} -->
<!-- ::: {.column width="100%"} -->
<!-- +-------------------------------+----------+---------------+--------------+-----------------+
| æ•™ç§‘æ›¸å                      |æ–‡ç« æ•°    |å»¶ã¹èªæ•°       |å¹³å‡å»¶ã¹èªæ•°  |å¹³å‡ç•°ãªã‚Šèªæ•°   |
+===============================+==========+===============+==============+=================+
| ä¸­ç´šæ—¥æœ¬èª ã‚«ãƒ«ãƒ†ãƒƒãƒˆ I       | 30       |         11861 |          395 |             157 |
+-------------------------------+----------+---------------+--------------+-----------------+
| ä¸­ç´šæ—¥æœ¬èª ã‚«ãƒ«ãƒ†ãƒƒãƒˆ II      | 29       |         17155 |          591 |             217 |
+-------------------------------+----------+---------------+--------------+-----------------+
| å­¦ã¼ã†! ã«ã»ã‚“ã” åˆä¸­ç´š       | 40       |          9057 |          226 |             100 |
+-------------------------------+----------+---------------+--------------+-----------------+
| å­¦ã¼ã†! ã«ã»ã‚“ã” ä¸­ç´š         | 40       |         15794 |          394 |             154 |
+-------------------------------+----------+---------------+--------------+-----------------+
| æ–°ç‰ˆ ä¸­æ—¥äº¤æµæ¨™æº–æ—¥æœ¬èª ä¸­ç´šä¸Š| 32       |         10372 |          324 |             128 |
+-------------------------------+----------+---------------+--------------+-----------------+
| æ–°ç‰ˆ ä¸­æ—¥äº¤æµæ¨™æº–æ—¥æœ¬èª ä¸­ç´šä¸‹| 32       |         13933 |          435 |             174 |
+-------------------------------+----------+---------------+--------------+-----------------+
 -->


<!-- ::: -->
<!-- :::
::: -->

:::footer
:file_folder:
:::

## æ–¹æ³•ğŸ”  {auto-animate="true" .smaller}
### 1.ã€€å¾“æ¥æŒ‡æ¨™ã‚’æ•™ç§‘æ›¸ã‚³ãƒ¼ãƒ‘ã‚¹ã«ã‚ã¦ã‚‹

- å½¢æ…‹ç´ è§£æãƒ„ãƒ¼ãƒ«ï¼ˆ[Fugashi](https://github.com/polm/fugashi)ï¼‰ã‚’ä½¿ç”¨
- å¾“æ¥æŒ‡æ¨™ã¨æ—¥æœ¬èªãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£å…¬å¼ã‚’è¨ˆç®—ã™ã‚‹

```{python}
# pip install --no-binary fugashi fugashi
import matplotlib.pyplot as plt
import pandas as pd
import re
from statistics import stdev
import math

def __ttr(words):

    return len(set(words)) / len(words)

def __sttr(wordlist, winsize=50):
 
    results = []
    for i in range(int(len(wordlist)/winsize)):
        results.append(__ttr(wordlist[i*winsize:(i*winsize)+winsize]))
    return mean(results)
```


## æ–¹æ³•ğŸ”  {auto-animate="true" .smaller}
### 1.ã€€å¾“æ¥æŒ‡æ¨™ã‚’æ•™ç§‘æ›¸ã‚³ãƒ¼ãƒ‘ã‚¹ã«ã‚ã¦ã‚‹
- å¾“æ¥æŒ‡æ¨™ã¨æ—¥æœ¬èªãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£å…¬å¼ã‚’è¨ˆç®—ã™ã‚‹
```{python}
def get_jp_sent(my_data: str) -> int:

    sent_detector = nltk.RegexpTokenizer(u'[^ã€€ï¼ï¼Ÿã€‚]*[ï¼ï¼Ÿã€‚.\n]')
    sents = sent_detector.tokenize(my_data)
    num = len(sents)
    return num


def get_jp_chars(my_data: str) -> int:

    chars = "".join(my_data)
    num_char = len(chars)

    return num_char



def calcARI_jp(my_data: str, debug=False) -> float:

    try:
        # score_ARI = 21.43*(get_jp_chars(my_data))/len(get_jp_tok(my_data))+0.5*(len(get_jp_tok(my_data))/text_to_sentence(my_data))-21.43
        chars = get_jp_chars(my_data)
        tokens = len(get_jp_tok(my_data))
        sentences = get_jp_sent(my_data)
        score_ARI = 21.43*chars/tokens + 0.5*tokens/sentences - 21.43

        if debug:
            print(f"chars={chars}, tokens={tokens}, sentences={sentences}, socre_ARI={score_ARI}")

        return score_ARI
    except ZeroDivisionError:
        return 0



def calcratio_jp(my_data: str) -> float:

    con_lst = ['åè©', 'å‰¯è©', 'å½¢å®¹è©', 'å‹•è©', 'å½¢çŠ¶è©']
    fun_lst = ['åŠ©å‹•è©', 'åŠ©è©']
    num1 = 0
    num2 = 0
    tokens = tagger(my_data)

    try:
        for word in tokens:
            if word.pos.split(',')[0] in con_lst:
                num1 += 1

        for word in tokens:
            if word.pos.split(',')[0] in fun_lst:
                num2 += 1

        return num1/len(tokens)*100

    except ZeroDivisionError:
        return 0

# Lee-readability ã¨ Shibazaki-redability ã®è¨ˆç®—å¼


def get_jp_chars(my_data: str) -> int:

    chars = "".join(my_data)
    num_char = len(chars)

    return num_char


def get_jp_tok(my_data: str) -> list[str]:

    tokens = tagger(my_data)
    token_strings = [token.surface for token in tokens]

    return token_strings


def text_to_sentence(text: str) -> list[str]:
    sent_detector = nltk.RegexpTokenizer(u'[^ã€€ï¼ï¼Ÿã€‚]*[ï¼ï¼Ÿã€‚.\n]')
    sents = sent_detector.tokenize(text)
    return sents



from statistics import mean

def calcLee(my_data: str, debug=False) -> float:

    sents = text_to_sentence(my_data)

    try:
        sentence_lengths = []
        for s in sents:
            tokens_s = tagger(s)
            sentence_lengths.append(len(tokens_s))

        avechar = mean(sentence_lengths)

        kannum = 0
        wanum = 0
        dnum = 0
        jnum = 0

        tokens = tagger(my_data)

        for token in tokens:
            if token.feature.goshu == 'æ¼¢':
                kannum += 1

        for token in tokens:
            if token.feature.goshu == 'å’Œ':
                wanum += 1

        for word in tokens:
            if word.pos.split(',')[0] == 'å‹•è©':
                dnum += 1

        for word in tokens:
            if word.pos.split(',')[0] == 'åŠ©è©':
                jnum += 1

        toknum = len(get_jp_tok(my_data))
        # score = 11.724-0.056*avechar-0.126*(kannum/toknum)-0.042*(wanum/toknum)-0.145*(dnum/toknum)-0.044*(jnum/toknum)
        score = 11.724-0.056*avechar-12.6*(kannum/toknum)-4.2*(wanum/toknum)-14.5*(dnum/toknum)-4.4*(jnum/toknum)

        if debug:
            print(f"avechar={avechar}, kannum={kannum}, wanum={wanum}, dnum={dnum}, jnum={jnum}, toknum={toknum}, score={score}")

        return score

    except ZeroDivisionError:
        return 0





def calc_shibazaki_chunk(my_data: str) -> int:

    n = 0
    # for i in my_data:
    chunk = bunsetu_spans(nlp(my_data))
    n = len(chunk)
    n += n

    return n





KATAKANA = set(list('ã‚¡ã‚¢ã‚£ã‚¤ã‚¥ã‚¦ã‚§ã‚¨ã‚©ã‚ªã‚«ã‚¬ã‚­ã‚®ã‚¯ã‚°ã‚±ã‚²ã‚³ã‚´ã‚µã‚¶ã‚·ã‚¸ã‚¹ã‚ºã‚»ã‚¼ã‚½'
                    'ã‚¾ã‚¿ãƒ€ãƒãƒ‚ãƒƒãƒ„ãƒ…ãƒ†ãƒ‡ãƒˆãƒ‰ãƒŠãƒ‹ãƒŒãƒãƒãƒãƒãƒ‘ãƒ’ãƒ“ãƒ”ãƒ•ãƒ–ãƒ—ãƒ˜ãƒ™ãƒš'
                    'ãƒ›ãƒœãƒãƒãƒŸãƒ ãƒ¡ãƒ¢ãƒ£ãƒ¤ãƒ¥ãƒ¦ãƒ§ãƒ¨ãƒ©ãƒªãƒ«ãƒ¬ãƒ­ãƒ¯ãƒ²ãƒ³ãƒ¼ãƒ®ãƒ°ãƒ±ãƒµãƒ¶ãƒ´'))

HIRAGANA = set(list('ãã‚ãƒã„ã…ã†ã‡ãˆã‰ãŠã‹ãŒããããã‘ã’ã“ã”ã•ã–ã—ã˜ã™ãš'
                    'ã›ãœãããŸã ã¡ã¢ã£ã¤ã¥ã¦ã§ã¨ã©ãªã«ã¬ã­ã®ã¯ã°ã±ã²ã³ã´'
                    'ãµã¶ã·ã¸ã¹ãºã»ã¼ã½ã¾ã¿ã‚€ã‚ã‚‚ã‚ƒã‚„ã‚…ã‚†ã‚‡ã‚ˆã‚‰ã‚Šã‚‹ã‚Œã‚ã‚'
                    'ã‚’ã‚“ãƒ¼ã‚ã‚ã‚‘ã‚•ã‚–ã‚”'))

KANJI_RX = re.compile(r'[\u4e00-\u9fff]')

from collections import defaultdict
def count_character_types(text):
    cmap = {
        'katakana': 0,
        'hiragana': 0,
        'kanji': 0,
        'other': 0,
    }
    katakana = 0
    hiragana = 0
    kanji = 0
    other = 0

    for c in text:
        if c in KATAKANA:
            katakana += 1
        elif c in HIRAGANA:
            hiragana += 1
        elif KANJI_RX.match(c):
            kanji += 1
        else:
            other += 1

    return kanji, hiragana, katakana, other





def calc_shibazaki_hiragana_ratio(my_data: str) -> float:

    ratio = count_character_types(my_data)[1] / len(my_data)

    return ratio


def calc_shibazaki_predicates(my_data: str) -> int:
    """
    Predicate countï¼ˆè¿°èªæ•°ï¼‰.
    """

    chunks = bunsetu_spans(nlp(my_data))
    predicates = 0
    for chunk in chunks:
        found = False
        for token in chunk:
            pos = token.tag_.split("-")
            pos_1 = pos[0]
            if len(pos) > 1:
                pos_2 = pos[1]
            else:
                pos_2 = ""
            if (re.search(r"çµ‚æ­¢å½¢-ä¸€èˆ¬", inflection(token)) or
                pos_1 == "å‹•è©" or
                (pos_1 == "åŠ©å‹•è©" and token.text != "ãª" and
                 any(x == token.lemma_ for x in ["ã ", "ã§ã™"])) or
                pos_2 == "çµ‚åŠ©è©" or
                pos_2 == "å¥ç‚¹"):
                found = True
        if found:
            predicates += 1

    return predicates




def calc_shibazaki_readability1(my_data: str, debug=False) -> float:
    sents = text_to_sentence(my_data)
    sentences = len(sents)
    hiragana_ratio = calc_shibazaki_hiragana_ratio(my_data) * 100 #ã€€Shibazaki_2014ã®è¡¨ï¼‘ã«å‚ç…§ã€å¹³ä»®åã®å‰²åˆã¯0ä»¥ä¸Šã®æœ‰ç†æ•°ã§è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ãŸã‚ã“ã“ã¯100ã‚’ã‹ã‘ã‚‹
    predicates = calc_shibazaki_predicates(my_data) / sentences


    year = + 14.016 \
           - (0.145 * hiragana_ratio) \
           + (0.587 * predicates)

    if debug:
        print(f"hiragana_ratio={hiragana_ratio}, predicates={predicates}, year={year}")

    return year

    
```


:::footer
:bar_chart:
:::

## çµæœğŸ“ {.smaller}

```{python}
import glob

readability = []
for file_name in glob.glob(r"/textbook-data/*.txt"):
    with open(file_name) as f:
        text = f.read()
        clean(text)
    tokens = get_jp_tok(text)
    measures = {
        "Name" : file_name.rpartition('/')[-1],
        "Textbook name" : file_name.split('/')[-2],
        "TTR" : __ttr(tokens),
        "STTR" : __sttr(tokens),
        "ARI" : calcARI_jp(text),
        "Lexical density" : calcratio_jp(text),
        "Lee-formula" : calcLee(text),
        "Shibazaki-formula-1" : calc_shibazaki_readability1(text) # æŸ´å´ãƒ»ç‰å²¡(2010)ã«ã‚ˆã‚‹ãƒªãƒ¼ãƒ€ãƒ“ãƒªãƒ†ã‚£å€¤, ç¯„å›²ã¯ãªã‚‹ã¹ãè¡¨ï¼‘ã¨ä¸€è‡´ã™ã¹ã
    }
    readability.append(measures)

test_df = pd.DataFrame(readability)

test_df
```

:::footer
:bar_chart:
:::

## çµæœğŸ“ {.smaller}

- æŒ‡æ¨™ã®ç›¸é–¢

```{python}
#| echo: true

test_df.corr()
```



:::footer
:bar_chart:
:::
## ã¾ã¨ã‚ğŸ“‹



## å‚è€ƒæ–‡çŒ® {.scrollable}

::: {.r-fit-text}
è¿‘è—¤é™½ä»‹, æ¾å‰ä¿Š, ä½è—¤ç†å², (2008):ã€Œæ•™ç§‘æ›¸ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ç”¨ã„ãŸæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®é›£æ˜“åº¦æ¨å®šã€:ã€è¨€èªå‡¦ç†å­¦ä¼šç¬¬ 14 å›å¹´æ¬¡å¤§ä¼šç™ºè¡¨è«–æ–‡é›†ã€. è¨€èªå‡¦ç†å­¦ä¼šç¬¬ 14 å›å¹´æ¬¡å¤§ä¼š: 1113-1116

æåœ¨é¬, (2016):ã€Œæ—¥æœ¬èªæ•™è‚²ã®ãŸã‚ã®æ–‡ç« é›£æ˜“åº¦ã«é–¢ã™ã‚‹ç ”ç©¶ã€. æ—©ç¨²ç”°æ—¥æœ¬èªæ•™è‚²å­¦ = Waseda studies in Japanese language education, (21): 1-16

æåœ¨é¬, (2019):ã€ŒBCCWJ ã«å«ã¾ã‚Œã‚‹å­¦æ ¡æ•™ç§‘æ›¸ã‚³ãƒ¼ãƒ‘ã‚¹ã®è¨ˆé‡çš„åˆ†æã€. è¨ˆé‡å›½èªå­¦, 32: (3): 147-162
Benjamin, R. G., (2012): â€œReconstructing Readability: Recent Developments and Recommendations in the Analysis of Text Difficulty.â€ Educ Psychol Rev, 24: 63-88. DOI: 10.1007/s10648-011-9181-8

HodoÅ¡Äek, B., Abekawa, T., Murota, M., Nishina, K., (2012): â€œReadability of example sentences in writing assistance tool Natsume.â€ In: 5th International Conference on Computer Assisted Systems For Teaching & Learning Japanese (CASTEL/J). Nagoya, Japan: 1-4

Lee, J h., HASEBE, Y., (2020): â€œReadability measurement of Japanese texts based on levelled corpora.â€ The Japanese Language from an Empirical Perspective, 143-168

Martinc, M., Pollak, S., Robnik-Å ikonja, M., (2021): â€œSupervised and Unsupervised Neural Approaches to Text Readability.â€ Computational Linguistics, 47: (1): 141-179. DOI: 10.1162/coli_a_00398

Shibasaki, H., Hara, S., (2010): â€œJapanese Readability Formula for Discriminating K-12 Grade Level Texts.â€ Mathematical Linguistics, 27: (6): 215-232

Tanaka-Ishii, K., Tezuka, S., Terada, H., (2010): â€œSorting Texts by Readability.â€ Computational Linguistics, 36: (2): 203-227
:::

:::footer
:book:
:::